{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c72c14",
   "metadata": {},
   "source": [
    "# Next Word Prediction Model using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb36572",
   "metadata": {},
   "source": [
    "Next Word Prediction means predicting the most likely word or phrase that will come next in a sentence or text. It is like having an inbuilt feature on an application that suggests the next word as you type or speak. The Next Word Prediction Models are used in applications like messaging apps, search engines, virtual assistants, and autocorrect features on smartphones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625dde83",
   "metadata": {},
   "source": [
    "### To build a Next Word Prediction model:\n",
    "1. start by collecting a diverse dataset of text documents, \n",
    "2. preprocess the data by cleaning and tokenizing it, \n",
    "3. prepare the data by creating input-output pairs, \n",
    "4. engineer features such as word embeddings, \n",
    "5. select an appropriate model like an LSTM or GPT, \n",
    "6. train the model on the dataset while adjusting hyperparameters,\n",
    "7. improve the model by experimenting with different techniques and architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6199b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "# Read the text file\n",
    "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c0fa4",
   "metadata": {},
   "source": [
    "# Tokenize the text to  create sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b3dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb56b19",
   "metadata": {},
   "source": [
    "In the above code, the text is tokenized, which means it is divided into individual words or tokens. The ‘Tokenizer’ object is created, which will handle the tokenization process. The ‘fit_on_texts’ method of the tokenizer is called, passing the ‘text’ as input. This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index. The ‘total_words’ variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62280ca8",
   "metadata": {},
   "source": [
    "create input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086eb4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3ad3e",
   "metadata": {},
   "source": [
    "The text data is split into lines using the ‘\\n’ character as a delimiter. For each line in the text, the ‘texts_to_sequences’ method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary. The resulting token list is then iterated over using a for loop. For each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning of the token list up to the current index ‘i’.\n",
    "\n",
    "This n-gram sequence represents the input context, with the last token being the target or predicted word. This n-gram sequence is then appended to the ‘input_sequences’ list. This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe3ce9",
   "metadata": {},
   "source": [
    "pad the input sequences to have equal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fd9fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef670d",
   "metadata": {},
   "source": [
    "The input sequences are padded to ensure all sequences have the same length. The variable ‘max_sequence_len’ is assigned the maximum length among all the input sequences. The ‘pad_sequences’ function is used to pad or truncate the input sequences to match this maximum length.\n",
    "\n",
    "The ‘pad_sequences’ function takes the input_sequences list, sets the maximum length to ‘max_sequence_len’, and specifies that the padding should be added at the beginning of each sequence using the ‘padding=pre’ argument. Finally, the input sequences are converted into a numpy array to facilitate further processing.\n",
    "\n",
    "Now let’s split the sequences into input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82790c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7dd313",
   "metadata": {},
   "source": [
    "The ‘X’ array is assigned the values of all rows in the ‘input_sequences’ array except for the last column. It means that ‘X’ contains all the tokens in each sequence except for the last one, representing the input context.\n",
    "\n",
    "On the other hand, the ‘y’ array is assigned the values of the last column in the ‘input_sequences’ array, which represents the target or predicted word.\n",
    "\n",
    "Now let’s convert the output to one-hot encode vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b05336",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847831c",
   "metadata": {},
   "source": [
    " we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6695abc",
   "metadata": {},
   "source": [
    "# Neural network architecture to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d25e9bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 250)               351000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8200)              2058200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3229200 (12.32 MB)\n",
      "Trainable params: 3229200 (12.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(250))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c1dad",
   "metadata": {},
   "source": [
    "The code above defines the model architecture for the next word prediction model. The ‘Sequential’ model is created, which represents a linear stack of layers. The first layer added to the model is the ‘Embedding’ layer, which is responsible for converting the input sequences into dense vectors of fixed size. \n",
    "It takes three arguments:\n",
    "\n",
    "1. ‘total_words’, which represents the total number of distinct words in the vocabulary; \n",
    "2. ‘100’, which denotes the dimensionality of the word embeddings; \n",
    "3. ‘input_length’, which specifies the length of the input sequences.\n",
    "\n",
    "The next layer added is the ‘LSTM’ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 250 internal representations or memory cells.\n",
    "\n",
    "Finally, the ‘Dense’ layer is added, which is a fully connected layer that produces the output predictions. It has ‘total_words’ units and uses the ‘softmax’ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c082ef9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ansh\\anaconda3\\conda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\ansh\\anaconda3\\conda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ansh\\anaconda3\\conda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3010/3010 [==============================] - 146s 47ms/step - loss: 6.1685 - accuracy: 0.0822\n",
      "Epoch 2/100\n",
      "3010/3010 [==============================] - 142s 47ms/step - loss: 5.4251 - accuracy: 0.1298\n",
      "Epoch 3/100\n",
      "3010/3010 [==============================] - 152s 50ms/step - loss: 5.0044 - accuracy: 0.1518\n",
      "Epoch 4/100\n",
      "3010/3010 [==============================] - 148s 49ms/step - loss: 4.6094 - accuracy: 0.1728\n",
      "Epoch 5/100\n",
      "3010/3010 [==============================] - 149s 49ms/step - loss: 4.2155 - accuracy: 0.1969\n",
      "Epoch 6/100\n",
      "3010/3010 [==============================] - 141s 47ms/step - loss: 3.8269 - accuracy: 0.2327\n",
      "Epoch 7/100\n",
      "3010/3010 [==============================] - 143s 48ms/step - loss: 3.4592 - accuracy: 0.2810\n",
      "Epoch 8/100\n",
      "3010/3010 [==============================] - 1180s 392ms/step - loss: 3.1207 - accuracy: 0.3318\n",
      "Epoch 9/100\n",
      "3010/3010 [==============================] - 135s 45ms/step - loss: 2.8162 - accuracy: 0.3862\n",
      "Epoch 10/100\n",
      "3010/3010 [==============================] - 138s 46ms/step - loss: 2.5388 - accuracy: 0.4364\n",
      "Epoch 11/100\n",
      "3010/3010 [==============================] - 143s 47ms/step - loss: 2.2885 - accuracy: 0.4876\n",
      "Epoch 12/100\n",
      "3010/3010 [==============================] - 138s 46ms/step - loss: 2.0625 - accuracy: 0.5367\n",
      "Epoch 13/100\n",
      "3010/3010 [==============================] - 144s 48ms/step - loss: 1.8641 - accuracy: 0.5795\n",
      "Epoch 14/100\n",
      "3010/3010 [==============================] - 143s 47ms/step - loss: 1.6852 - accuracy: 0.6186\n",
      "Epoch 15/100\n",
      "3010/3010 [==============================] - 142s 47ms/step - loss: 1.5270 - accuracy: 0.6555\n",
      "Epoch 16/100\n",
      "3010/3010 [==============================] - 127s 42ms/step - loss: 1.3855 - accuracy: 0.6890\n",
      "Epoch 17/100\n",
      "3010/3010 [==============================] - 125s 42ms/step - loss: 1.2659 - accuracy: 0.7152\n",
      "Epoch 18/100\n",
      "3010/3010 [==============================] - 134s 44ms/step - loss: 1.1569 - accuracy: 0.7414\n",
      "Epoch 19/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 1.0625 - accuracy: 0.7607\n",
      "Epoch 20/100\n",
      "3010/3010 [==============================] - 125s 42ms/step - loss: 0.9859 - accuracy: 0.7794\n",
      "Epoch 21/100\n",
      "3010/3010 [==============================] - 126s 42ms/step - loss: 0.9144 - accuracy: 0.7947\n",
      "Epoch 22/100\n",
      "3010/3010 [==============================] - 123s 41ms/step - loss: 0.8542 - accuracy: 0.8084\n",
      "Epoch 23/100\n",
      "3010/3010 [==============================] - 124s 41ms/step - loss: 0.8047 - accuracy: 0.8189\n",
      "Epoch 24/100\n",
      "3010/3010 [==============================] - 142s 47ms/step - loss: 0.7602 - accuracy: 0.8284\n",
      "Epoch 25/100\n",
      "3010/3010 [==============================] - 128s 42ms/step - loss: 0.7248 - accuracy: 0.8351\n",
      "Epoch 26/100\n",
      "3010/3010 [==============================] - 119s 40ms/step - loss: 0.6951 - accuracy: 0.8418\n",
      "Epoch 27/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 0.6703 - accuracy: 0.8463\n",
      "Epoch 28/100\n",
      "3010/3010 [==============================] - 139s 46ms/step - loss: 0.6424 - accuracy: 0.8531\n",
      "Epoch 29/100\n",
      "3010/3010 [==============================] - 135s 45ms/step - loss: 0.6225 - accuracy: 0.8558\n",
      "Epoch 30/100\n",
      "3010/3010 [==============================] - 136s 45ms/step - loss: 0.6059 - accuracy: 0.8589\n",
      "Epoch 31/100\n",
      "3010/3010 [==============================] - 128s 43ms/step - loss: 0.5954 - accuracy: 0.8603\n",
      "Epoch 32/100\n",
      "3010/3010 [==============================] - 128s 43ms/step - loss: 0.5863 - accuracy: 0.8603\n",
      "Epoch 33/100\n",
      "3010/3010 [==============================] - 133s 44ms/step - loss: 0.5783 - accuracy: 0.8621\n",
      "Epoch 34/100\n",
      "3010/3010 [==============================] - 128s 43ms/step - loss: 0.5623 - accuracy: 0.8658\n",
      "Epoch 35/100\n",
      "3010/3010 [==============================] - 131s 44ms/step - loss: 0.5582 - accuracy: 0.8646\n",
      "Epoch 36/100\n",
      "3010/3010 [==============================] - 136s 45ms/step - loss: 0.5538 - accuracy: 0.8663\n",
      "Epoch 37/100\n",
      "3010/3010 [==============================] - 135s 45ms/step - loss: 0.5475 - accuracy: 0.8681\n",
      "Epoch 38/100\n",
      "3010/3010 [==============================] - 140s 47ms/step - loss: 0.5422 - accuracy: 0.8671\n",
      "Epoch 39/100\n",
      "3010/3010 [==============================] - 134s 44ms/step - loss: 0.5337 - accuracy: 0.8690\n",
      "Epoch 40/100\n",
      "3010/3010 [==============================] - 139s 46ms/step - loss: 0.5325 - accuracy: 0.8672\n",
      "Epoch 41/100\n",
      "3010/3010 [==============================] - 142s 47ms/step - loss: 0.5247 - accuracy: 0.8700\n",
      "Epoch 42/100\n",
      "3010/3010 [==============================] - 142s 47ms/step - loss: 0.5258 - accuracy: 0.8691\n",
      "Epoch 43/100\n",
      "3010/3010 [==============================] - 139s 46ms/step - loss: 0.5224 - accuracy: 0.8698\n",
      "Epoch 44/100\n",
      "3010/3010 [==============================] - 139s 46ms/step - loss: 0.5196 - accuracy: 0.8694\n",
      "Epoch 45/100\n",
      "3010/3010 [==============================] - 140s 46ms/step - loss: 0.5190 - accuracy: 0.8700\n",
      "Epoch 46/100\n",
      "3010/3010 [==============================] - 141s 47ms/step - loss: 0.5165 - accuracy: 0.8690\n",
      "Epoch 47/100\n",
      "3010/3010 [==============================] - 141s 47ms/step - loss: 0.5110 - accuracy: 0.8708\n",
      "Epoch 48/100\n",
      "3010/3010 [==============================] - 131s 44ms/step - loss: 0.5148 - accuracy: 0.8690\n",
      "Epoch 49/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 0.5116 - accuracy: 0.8700\n",
      "Epoch 50/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 0.5088 - accuracy: 0.8697\n",
      "Epoch 51/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 0.5042 - accuracy: 0.8714\n",
      "Epoch 52/100\n",
      "3010/3010 [==============================] - 128s 43ms/step - loss: 0.5073 - accuracy: 0.8696\n",
      "Epoch 53/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 0.5014 - accuracy: 0.8711\n",
      "Epoch 54/100\n",
      "3010/3010 [==============================] - 128s 43ms/step - loss: 0.5092 - accuracy: 0.8683\n",
      "Epoch 55/100\n",
      "3010/3010 [==============================] - 128s 43ms/step - loss: 0.5044 - accuracy: 0.8701\n",
      "Epoch 56/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 0.5035 - accuracy: 0.8694\n",
      "Epoch 57/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 0.5025 - accuracy: 0.8701\n",
      "Epoch 58/100\n",
      "3010/3010 [==============================] - 160s 53ms/step - loss: 0.5039 - accuracy: 0.8684\n",
      "Epoch 59/100\n",
      "3010/3010 [==============================] - 125s 41ms/step - loss: 0.5019 - accuracy: 0.8683\n",
      "Epoch 60/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 0.5061 - accuracy: 0.8670\n",
      "Epoch 61/100\n",
      "3010/3010 [==============================] - 122s 41ms/step - loss: 0.5079 - accuracy: 0.8667\n",
      "Epoch 62/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 0.5017 - accuracy: 0.8688\n",
      "Epoch 63/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5025 - accuracy: 0.8680\n",
      "Epoch 64/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 0.4970 - accuracy: 0.8701\n",
      "Epoch 65/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 0.5087 - accuracy: 0.8664\n",
      "Epoch 66/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.5029 - accuracy: 0.8666\n",
      "Epoch 67/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.4961 - accuracy: 0.8693\n",
      "Epoch 68/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 0.5075 - accuracy: 0.8652\n",
      "Epoch 69/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5039 - accuracy: 0.8666\n",
      "Epoch 70/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.5011 - accuracy: 0.8670\n",
      "Epoch 71/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5059 - accuracy: 0.8662\n",
      "Epoch 72/100\n",
      "3010/3010 [==============================] - 113s 38ms/step - loss: 0.5049 - accuracy: 0.8650\n",
      "Epoch 73/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5005 - accuracy: 0.8667\n",
      "Epoch 74/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5069 - accuracy: 0.8646\n",
      "Epoch 75/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5047 - accuracy: 0.8650\n",
      "Epoch 76/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5003 - accuracy: 0.8665\n",
      "Epoch 77/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5015 - accuracy: 0.8660\n",
      "Epoch 78/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5009 - accuracy: 0.8664\n",
      "Epoch 79/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5058 - accuracy: 0.8644\n",
      "Epoch 80/100\n",
      "3010/3010 [==============================] - 113s 38ms/step - loss: 0.5023 - accuracy: 0.8647\n",
      "Epoch 81/100\n",
      "3010/3010 [==============================] - 113s 37ms/step - loss: 0.5039 - accuracy: 0.8640\n",
      "Epoch 82/100\n",
      "3010/3010 [==============================] - 113s 37ms/step - loss: 0.5022 - accuracy: 0.8649\n",
      "Epoch 83/100\n",
      "3010/3010 [==============================] - 113s 37ms/step - loss: 0.5020 - accuracy: 0.8646\n",
      "Epoch 84/100\n",
      "3010/3010 [==============================] - 113s 37ms/step - loss: 0.5062 - accuracy: 0.8638\n",
      "Epoch 85/100\n",
      "3010/3010 [==============================] - 113s 37ms/step - loss: 0.5078 - accuracy: 0.8632\n",
      "Epoch 86/100\n",
      "3010/3010 [==============================] - 113s 38ms/step - loss: 0.5057 - accuracy: 0.8633\n",
      "Epoch 87/100\n",
      "3010/3010 [==============================] - 113s 38ms/step - loss: 0.5064 - accuracy: 0.8625\n",
      "Epoch 88/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 0.5049 - accuracy: 0.8631\n",
      "Epoch 89/100\n",
      "3010/3010 [==============================] - 113s 38ms/step - loss: 0.5067 - accuracy: 0.8620\n",
      "Epoch 90/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 0.5048 - accuracy: 0.8645\n",
      "Epoch 91/100\n",
      "3010/3010 [==============================] - 113s 37ms/step - loss: 0.5073 - accuracy: 0.8617\n",
      "Epoch 92/100\n",
      "3010/3010 [==============================] - 113s 37ms/step - loss: 0.5142 - accuracy: 0.8606\n",
      "Epoch 93/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 0.5067 - accuracy: 0.8633\n",
      "Epoch 94/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5152 - accuracy: 0.8591\n",
      "Epoch 95/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 0.5150 - accuracy: 0.8599\n",
      "Epoch 96/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 0.5110 - accuracy: 0.8603\n",
      "Epoch 97/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 0.5106 - accuracy: 0.8609\n",
      "Epoch 98/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 0.5095 - accuracy: 0.8606\n",
      "Epoch 99/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 0.5193 - accuracy: 0.8576\n",
      "Epoch 100/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 0.5130 - accuracy: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x285e71b0310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c85c331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "I will leave if they not come to you are so far from the same as to this of the obvious then i took my attention to the smoke and his face of crime it rose to be a colonel rushed place the other into the other as i have ever and much in the\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I will leave if they\"\n",
    "next_words = 50\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df702b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
